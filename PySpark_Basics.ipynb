{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required libraries and setting up dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bin   data\tjars\t    LICENSE   NOTICE  R\t\t RELEASE  yarn\r\n",
      "conf  examples\tkubernetes  licenses  python  README.md  sbin\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"/usr/spark2.4.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "py4j-0.10.7-src.zip  PY4J_LICENSE.txt  pyspark.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"/usr/spark2.4.3/python/lib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SPARK_HOME\"] = \"/usr/spark2.4.3\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Single Source of Entrypoint in Spark 2.x or Higher \n",
    "- local : Run Spark locally with one worker thread (i.e. no parallelism at all).\n",
    "- local[K] : Run Spark locally with K worker threads (ideally, set this to the number of cores on your machine). Eg.:\n",
    "- local[1] : This allocates only one CPU for tasks and if a receiver is running on it, there is no resource left to process the received data. \n",
    "- local[*] : Run Spark locally with as many worker threads as logical cores on your machine. It uses as many threads as your spark local machine have, where you are running your application.\n",
    "- local[*,F] : Run Spark locally with as many worker threads as logical cores on your machine and F maxFailures.   \n",
    "- yarn : Connect to a YARN cluster in client or cluster mode depending on the value of --deploy-mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Aman_Spark_First_App\").master(\"local[*]\").enableHiveSupport().getOrCreate()\n",
    "# master(\"yarn\")     # If we are using yarn/mesos as our Resource Manager , then pass 'yarn' as argument to master\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of our Spark Application is : <SparkContext master=local[*] appName=Aman_Spark_First_App> \n",
      "\n",
      "Name of our Spark Application is : Aman_Spark_First_App \n",
      "\n",
      "Name of our Spark object is : <pyspark.sql.session.SparkSession object at 0x7fbe70189860> \n",
      "\n",
      "Spark Version is : 2.4.3 \n",
      "\n",
      "Directories available under Spark is \n",
      " : ['PACKAGE_EXTENSIONS', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getnewargs__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_accumulatorServer', '_active_spark_context', '_batchSize', '_callsite', '_checkpointFile', '_conf', '_dictToJavaMap', '_do_init', '_encryption_enabled', '_ensure_initialized', '_gateway', '_getJavaStorageLevel', '_initialize_context', '_javaAccumulator', '_jsc', '_jvm', '_lock', '_next_accum_id', '_pickled_broadcast_vars', '_python_includes', '_repr_html_', '_serialize_to_jvm', '_temp_dir', '_unbatched_serializer', 'accumulator', 'addFile', 'addPyFile', 'appName', 'applicationId', 'binaryFiles', 'binaryRecords', 'broadcast', 'cancelAllJobs', 'cancelJobGroup', 'defaultMinPartitions', 'defaultParallelism', 'dump_profiles', 'emptyRDD', 'environment', 'getConf', 'getLocalProperty', 'getOrCreate', 'hadoopFile', 'hadoopRDD', 'master', 'newAPIHadoopFile', 'newAPIHadoopRDD', 'parallelize', 'pickleFile', 'profiler_collector', 'pythonExec', 'pythonVer', 'range', 'runJob', 'sequenceFile', 'serializer', 'setCheckpointDir', 'setJobDescription', 'setJobGroup', 'setLocalProperty', 'setLogLevel', 'setSystemProperty', 'show_profiles', 'sparkHome', 'sparkUser', 'startTime', 'statusTracker', 'stop', 'textFile', 'uiWebUrl', 'union', 'version', 'wholeTextFiles'] \n",
      "\n",
      "Location of our Python directory is : /usr/local/anaconda/bin/python \n",
      "\n",
      "Anaconda verison is :  sys.version_info(major=3, minor=6, micro=8, releaselevel='final', serial=0) \n",
      "\n",
      "Python version is : 3.6.8 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Name of our Spark Application is : {sc} \\n')\n",
    "print(f'Name of our Spark Application is : {sc.appName} \\n')\n",
    "print(f'Name of our Spark object is : {spark} \\n')\n",
    "print(f'Spark Version is : {sc.version} \\n')\n",
    "print(f'Directories available under Spark is \\n : {dir(sc)} \\n')\n",
    "from platform import python_version\n",
    "print(f'Location of our Python directory is : {sys.executable} \\n')\n",
    "print(f'Anaconda verison is :  {sys.version_info} \\n') \n",
    "print(f'Python version is : {python_version()} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Python Vs Spark Datastructure Properties\n",
    "- Basic commands\n",
    "- Transformations modify an RDD (e.g. filter out some lines) and return an RDD\n",
    "- Actions modify an RDD and returns a Python object. After an action, we can use standard Python on these objects again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python List Properties \n",
      "\n",
      "[1, 2, 3, 4]\n",
      "<class 'list'>\n",
      "4\n",
      "*********************************************\n",
      "Spark RDD Properties \n",
      "\n",
      "12\n",
      "15\n",
      "ParallelCollectionRDD[40] at parallelize at PythonRDD.scala:195\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "[1, 2, 3, 4]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print('Python List Properties \\n')\n",
    "list1 = [1,2,3,4]\n",
    "print(list1)\n",
    "print(type(list1))\n",
    "print(len(list1))\n",
    "\n",
    "print('***'*15)\n",
    "\n",
    "print('Spark RDD Properties \\n')\n",
    "list1_int = sc.parallelize(list1)                   # Create RDD from python List\n",
    "print(list1_int.getNumPartitions())                 # Checking default partitions in RDD\n",
    "list1_int = sc.parallelize(list1,15)                # Specifying partitions in RDD\n",
    "print(list1_int.getNumPartitions())\n",
    "print(list1_int)\n",
    "print(type(list1_int))\n",
    "print(list1_int.collect())                          # Printing elements of RDD\n",
    "print(list1_int.count())                            # Printing COUNT of elements in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python List Properties \n",
      "\n",
      "['aman', 'deep', 'samra']\n",
      "<class 'list'>\n",
      "3\n",
      "*********************************************\n",
      "Spark RDD Properties \n",
      "\n",
      "12\n",
      "3\n",
      "ParallelCollectionRDD[37] at parallelize at PythonRDD.scala:195\n",
      "<class 'pyspark.rdd.RDD'>\n",
      "['aman', 'deep', 'samra']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print('Python List Properties \\n')\n",
    "list1 = ['aman','deep','samra']\n",
    "print(list1)\n",
    "print(type(list1))\n",
    "print(len(list1))\n",
    "\n",
    "print('***'*15)\n",
    "\n",
    "print('Spark RDD Properties \\n')\n",
    "list11_str = sc.parallelize(list1)                   # Create RDD from python List\n",
    "print(list11_str.getNumPartitions())                 # Checking default partitions in RDD\n",
    "list11_str = sc.parallelize(list1,3)                # Specifying partitions in RDD\n",
    "print(list11_str.getNumPartitions())\n",
    "print(list11_str)\n",
    "print(type(list11_str))\n",
    "print(list11_str.collect())                          # Printing elements of RDD\n",
    "print(list11_str.count())                            # Printing COUNT of elements in RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning \n",
    "- mapPartitions - Useful when we want processing/aggregation/computation at partition level Not at global level.\n",
    "- spark.default.parallelism - is the default number of partition set by spark which is by default 200\n",
    "- spark.sql.shuffle.partitions - to set number of partition in the spark configuration or while running spark SQL.\n",
    "- 'repartition' - shuffle the data in RDD as per requirement less or more. \n",
    "- Creates a new RDD and  does a full shuffle and creates new partitions with data that's distributed evenly. \n",
    "- New partitions are created. \n",
    "- Repartition works faster than shuffle because it creates even partition and spark is built to leverage that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python list has elements : [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "No. of partitions done on rdd is : 3\n",
      "Distibution of data on RDD via Partitioning : [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
      "No. of partitions AFTER Repartitioing  on rdd is : 4\n",
      "Distibution of data AFTER Re-Partitioning changes: [[6, 7, 8], [3, 4, 5], [], [0, 1, 2]]\n"
     ]
    }
   ],
   "source": [
    "python_list = [i for i in range(9)]\n",
    "print(f'Python list has elements : {python_list}')\n",
    "rdd = sc.parallelize(python_list,3)                   \n",
    "print(f'No. of partitions done on rdd is : {rdd.getNumPartitions()}')\n",
    "print(f'Distibution of data on RDD via Partitioning : {rdd.glom().collect()}')\n",
    "repartitoned_rdd = rdd.repartition(4)\n",
    "print(f'No. of partitions AFTER Repartitioing  on rdd is : {repartitoned_rdd.getNumPartitions()}')\n",
    "print(f'Distibution of data AFTER Re-Partitioning changes: {repartitoned_rdd.glom().collect()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COALESCE \n",
    "- Creates a new RDD and allows repartitioning but we can only decrease the size of partitions as it avoids full data shuffling which is expensive process. \n",
    "- Executors only move the data from extra nodes. \n",
    "- If data is spread across 4 nodes and we coalesced it to 2 nodes , then only 2 nodes will be touched by executors. - - But unequal sized partitions are generally slower. \n",
    "- No new partitions are created, it’s basically kind of compression \n",
    "- But even if we increase the no. of partition than acutal core, it wont do anything "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10]\n",
      "No. of partitions done on rdd is : 3\n",
      "Actual distribution is : [[5, 6], [7, 8], [9, 10]]\n",
      "***************************************************************************\n",
      "No. of partitions done on rdd AFTER COALESCE is : 2\n",
      "COALESCE distribution is : [[5, 6], [7, 8, 9, 10]]\n",
      "***************************************************************************\n",
      "No. of partitions AFTER COALESCE > THAN ACTUAL CORE  on rdd is : 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[5, 6], [7, 8], [9, 10]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "python_list = [i for i in range(5,11)]\n",
    "print(python_list)\n",
    "rdd = sc.parallelize(python_list,3)  \n",
    "print(f'No. of partitions done on rdd is : {rdd.getNumPartitions()}')\n",
    "print(f'Actual distribution is : {rdd.glom().collect()}')\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "coalesced_rdd = rdd.coalesce(2)\n",
    "print(f'No. of partitions done on rdd AFTER COALESCE is : {coalesced_rdd.getNumPartitions()}')\n",
    "print(f'COALESCE distribution is : {coalesced_rdd.glom().collect()}')\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "coalesced_rdd_n = rdd.coalesce(4)\n",
    "print(f'No. of partitions AFTER COALESCE > THAN ACTUAL CORE  on rdd is : {coalesced_rdd_n.getNumPartitions()}')\n",
    "coalesced_rdd_n.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Functions :\n",
    "- MAP : Transform your data row-wise that means it iterate over each element of RDD, returns only 1 element and new RDD . One element in -> one element out. It returns a new RDD.\n",
    "- FLATMAP : Behaves similar like MAP ,also returns a new RDD with 0 or more than 1 elements at a time depends on developer Business logic, by applying function over each element and finally flatten the result i.e 1D. One element in -> 0 or more elements out (a collection). Really helpful with nested json structures. Its like unlist() in R programming.\n",
    "- FILTER - returns only the elements that satisfy the search filter, similar to WHERE clause of SQL\n",
    "\n",
    "- mapValues, flatMapValues - are More efficient than map and flatMap because Spark can maintain the partitioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 7, 8, 9]\n",
      "[1, 4, 9, 16]\n",
      "[3, 4]\n",
      "[2, 4]\n"
     ]
    }
   ],
   "source": [
    "print(list1_int.map(lambda x : x+5).collect())\n",
    "print(list1_int.map(lambda x : x**2).collect())\n",
    "print(list1_int.filter(lambda x : x > 2).collect())\n",
    "print(list1_int.filter(lambda x : x % 2 == 0).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[['aman'], ['deep'], ['samra']]\n",
      "[('aman', 1), ('deep', 1), ('samra', 1)]\n",
      "['Aman', 'Deep', 'Samra']\n",
      "['AMAN', 'DEEP', 'SAMRA']\n",
      "*********************************************\n",
      "['aman', 'samra']\n",
      "['aman', 'deep', 'samra']\n",
      "*********************************************\n",
      "['aman', 'deep', 'samra']\n",
      "['aman', 1, 'deep', 1, 'samra', 1]\n",
      "['A', 'm', 'a', 'n', 'D', 'e', 'e', 'p', 'S', 'a', 'm', 'r', 'a']\n"
     ]
    }
   ],
   "source": [
    "print(list11_str.zipWithIndex().lookup('deep'))\n",
    "print(list11_str.map(lambda x: x.split(',')).collect())     # Result is 2D structure\n",
    "print(list11_str.map(lambda x: (x,1)).collect())\n",
    "print(list11_str.map(lambda x:  x.title()).collect())\n",
    "print(list11_str.map(lambda x:  x.upper()).collect())\n",
    "\n",
    "print('***'*15)\n",
    "print(list11_str.filter(lambda x: 'a' in x).collect())\n",
    "print(list11_str.filter(lambda x: x.split(',')).collect())\n",
    "\n",
    "print('***'*15)\n",
    "print(list11_str.flatMap(lambda x: x.split(',')).collect())   # Result is flattened 1D structure\n",
    "print(list11_str.flatMap(lambda x: (x,1)).collect())\n",
    "print(list11_str.flatMap(lambda x:  x.title()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am', 'am applying', 'applying for', 'for Jr.', 'Jr. Research', 'Research Engineer']\n",
      "[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
      "[6, 15, 24]\n",
      "[[\"['I\", 'am', 'applying'], ['for', 'Jr.', 'Research'], [\"Engineer']\"]]\n",
      "[3, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# n_gram logic with n_gram = 2\n",
    "list1 = 'I am applying for Jr. Research Engineer'\n",
    "list1 = list1.split()\n",
    "\n",
    "n_gram = []\n",
    "for i in range(len(list1)-2+1):\n",
    "    tup = ' '.join(list1[i:i+2])\n",
    "    n_gram.append(tup)\n",
    "print(n_gram)\n",
    "\n",
    "\n",
    "# Break a list into partition and find the elements sum and len in each partition\n",
    "# num_partition = 3\n",
    "\n",
    "list1 = [1,2,3,4,5,6,7,8,9]\n",
    "sublist = []\n",
    "sum_sublist = []\n",
    "for x in range(0, len(list1),3):\n",
    "    sublist.append(list1[x:x+3])\n",
    "    sum_sublist.append(sum(list1[x:x+3]))\n",
    "print(sublist)\n",
    "print(sum_sublist)\n",
    "\n",
    "\n",
    "list1 = ['I am applying for Jr. Research Engineer']\n",
    "list1 = str(list1).split()\n",
    "sublist = []\n",
    "len_sublist = []\n",
    "for x in range(0, len(list1),3):\n",
    "    sublist.append(list1[x:x+3])\n",
    "    len_sublist.append(len(list1[x:x+3]))\n",
    "print(sublist)\n",
    "print(len_sublist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[368] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_parititon(rdd, n_partition):\n",
    "    sublist = []\n",
    "    sum_sublist = []\n",
    "    for x in range(0, len(rdd),n_partition):\n",
    "        sublist.append(rdd[x:x+n_partition])\n",
    "        sum_sublist.append(sum(rdd[x:x+n_partition]))\n",
    "    return sublist, sum_sublist\n",
    "\n",
    "list1 = [1,2,3,4,5,6,7,8,9]\n",
    "list1_int = sc.parallelize(list1)                   \n",
    "paritioned_rdd = list1_int.map(num_parititon(list1_int.collect(),3))\n",
    "paritioned_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8]\n",
      "[2, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(list1_int.sample(withReplacement=False , fraction=0.5, seed=1).collect())\n",
    "print(list1_int.sample(withReplacement=True , fraction=0.5, seed=1).collect())   # Can produce dupes liek 3 is sampled twice "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET OPERATORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Union Operator bring ALL Elements : \n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1, 2]\n",
      "18\n",
      "***************************************************************************\n",
      "Intersection Operator brings COMMON elements: \n",
      "[1, 2]\n",
      "2\n",
      "***************************************************************************\n",
      "DISTINCT brings UNIQUE elements: \n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,3,4,5,6,7,8,9]\n",
    "list2 = [10,11,12,13,14,15,16,1,2]\n",
    "print(len(list1)==len(list2))\n",
    "\n",
    "list1_int = sc.parallelize(list1)                   \n",
    "list2_int = sc.parallelize(list2)   \n",
    "\n",
    "print('Union Operator bring ALL Elements : ')\n",
    "union_rdd = list1_int.union(list2_int)\n",
    "print(union_rdd.collect())\n",
    "print(union_rdd.count())\n",
    "\n",
    "print('*****'*15)\n",
    "print('Intersection Operator brings COMMON elements: ')\n",
    "intersection_rdd = list1_int.intersection(list2_int)\n",
    "print(intersection_rdd.collect())\n",
    "print(intersection_rdd.count())\n",
    "\n",
    "print('*****'*15)\n",
    "print('DISTINCT brings UNIQUE elements: ')\n",
    "Unique_rdd = union_rdd.distinct()\n",
    "print(Unique_rdd.collect())\n",
    "print(Unique_rdd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group by key vs Reduce by key vs Sort by key\n",
    "- All functions on K:V pair RDD's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupByKey \n",
    "- Before shuffling, Data is not combined and transferred as it is. Costlier operation if there is more data in 1 partition than other. Doesn't use Map Reduce Combiner. Performace degradation due to lots of shuffling. Takes input as K:V Pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 2), (4, 1), (5, 1), (7, 2), (8, 1)]\n",
      "***************************************************************************\n",
      "[('python', 1), ('hello', 2), ('aman', 2), ('deep', 1), ('samra', 1)]\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,1,4,5,2,7,8,7]\n",
    "list1_int = sc.parallelize(list1)                   \n",
    "group_by_key_rdd = list1_int.map(lambda x : (x,1))  \\\n",
    "                            .groupByKey() \\\n",
    "                            .map(lambda x: (x[0], sum(x[1])))\n",
    "\n",
    "print(group_by_key_rdd.collect())\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "list1 = ['aman deep samra aman hello python hello']\n",
    "list1_str = sc.parallelize(list1)  \n",
    "group_by_key_rdd = list1_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,1))\\\n",
    "                            .groupByKey()\\\n",
    "                            .map(lambda x : (x[0], sum(x[1])))\n",
    "         \n",
    "print(group_by_key_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reduceByKey \n",
    "- Before shuffling, operation is done on partition first and then Data is combined  and then transferred to new Executors. It uses Map Reduce Combiner. Much faster than groupbykey because less shuffling is involved. Good performance. Takes input as K:V Pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (2, 2), (4, 1), (5, 1), (7, 2), (8, 1)]\n",
      "***************************************************************************\n",
      "[('python', 1), ('hello', 2), ('aman', 2), ('deep', 1), ('samra', 1)]\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,1,4,5,2,7,8,7]\n",
    "list1_int = sc.parallelize(list1)                   \n",
    "group_by_key_rdd = list1_int.map(lambda x : (x,1))\\\n",
    "                            .reduceByKey(lambda x,y : (x + y)) \n",
    "                            \n",
    "print(group_by_key_rdd.collect())\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "list1 = ['aman deep samra aman hello python hello']\n",
    "list1_str = sc.parallelize(list1)  \n",
    "group_by_key_rdd = list1_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,1))\\\n",
    "                            .reduceByKey(lambda x,y : (x + y)) \n",
    "         \n",
    "print(group_by_key_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sortByKey\n",
    "- simply sort the data by key in desc or asc order. By default its always ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 1), (7, 1), (7, 1), (5, 1), (4, 1), (2, 1), (2, 1), (1, 1), (1, 1)]\n",
      "***************************************************************************\n",
      "[('samra', 1), ('python', 1), ('hello', 1), ('hello', 1), ('deep', 1), ('aman', 1), ('aman', 1)]\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,1,4,5,2,7,8,7]\n",
    "list1_int = sc.parallelize(list1)                   \n",
    "group_by_key_rdd = list1_int.map(lambda x : (x,1))\\\n",
    "                            .sortByKey(ascending= False)\n",
    "\n",
    "print(group_by_key_rdd.collect())\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "list1 = ['aman deep samra aman hello python hello']\n",
    "list1_str = sc.parallelize(list1)  \n",
    "group_by_key_rdd = list1_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,1))\\\n",
    "                            .sortByKey(ascending= False)\n",
    "         \n",
    "print(group_by_key_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 64), (7, 49), (7, 49), (5, 25), (4, 16), (2, 4), (2, 4), (1, 1), (1, 1)]\n",
      "[(5, 25), (8, 64), (7, 49), (4, 16), (1, 1), (2, 4)]\n"
     ]
    }
   ],
   "source": [
    "list1 = [1,2,1,4,5,2,7,8,7]\n",
    "list1_int = sc.parallelize(list1)                   \n",
    "group_by_key_rdd = list1_int.map(lambda x : (x,x**2))\\\n",
    "                            .sortByKey(ascending= False)\n",
    "\n",
    "print(group_by_key_rdd.collect())\n",
    "\n",
    "distinct_rdd = group_by_key_rdd.distinct()\n",
    "print(distinct_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### aggregateByKey \n",
    "- aggregateByKey - Takes 3 arguments i.e Initializer/Accumulators , Combiner/Sequence Function and Final Merge function\n",
    "- Accumulators -  if sum then initialize with 0 , if multiply then initialize with 1\n",
    "- Combine/Sequence Function - Combine values within single partition. Gives intermediate result to Final merge function\n",
    "- Final merge Function - Combine values from all partitions, takes input from Combine Function\n",
    "- Value of output can be differnet from Input\n",
    "- reduceByKey is special variant of aggregateByKey\n",
    "- They both helps in Parallel computation as they perform the Combine function on their respective partitions\n",
    "- aggregateByKey can result in multiple datatypes but reduceByKey outputs the same datatype\n",
    "- Useful when we have to perform operation on values splitted by partiton followed by any aggregation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reduceByKey :\n",
      "[('aman', 80), ('saurabh', 140), ('manish', 60)]\n",
      "aggregateByKey exanple 1 and 2 :\n",
      "[('aman', 80), ('saurabh', 140), ('manish', 60)]\n",
      "[('aman', 80), ('saurabh', 140), ('manish', 60)]\n"
     ]
    }
   ],
   "source": [
    "list1 = ['aman saurabh manish aman saurabh']\n",
    "list1_str = sc.parallelize(list1)  \n",
    "group_by_key_rdd = list1_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,len(x)*10))\n",
    "\n",
    "print('reduceByKey :')\n",
    "print(group_by_key_rdd.reduceByKey(lambda x,y : (x + y)).collect())\n",
    "\n",
    "print('aggregateByKey exanple 1 and 2 :')\n",
    "print(group_by_key_rdd.aggregateByKey(0, lambda x,y : (x + y) , lambda y,x: x+y).collect())\n",
    "print(group_by_key_rdd.aggregateByKey(0, lambda k,v: int(v)+k, lambda v,k: k+v).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keyBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'aman'),\n",
       " ('s', 'saurabh'),\n",
       " ('m', 'manish'),\n",
       " ('d', 'deep'),\n",
       " ('c', 'clarke')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = ['aman saurabh manish deep clarke']\n",
    "\n",
    "list_rdd = sc.parallelize(list1)\n",
    "split_list_rdd = list_rdd.flatMap(lambda x : x.split(' '))   # Flattened 1D Array\n",
    "#list_rdd.map(lambda x : x.split(' ')).collect() # 2D Array \n",
    "split_list_rdd.keyBy(lambda x : x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python ZIP function results in : \n",
      " [('aman', 'samra'), ('saurabh', 'mishra'), ('manish', 'dhull')] \n",
      "\n",
      "Python ZIP function results in : \n",
      " ['aman samra', 'saurabh mishra', 'manish dhull'] \n",
      "\n",
      "************************************************************\n",
      "[PySpark RDD ZIP function results in : \n",
      " [('aman', 'samra'), ('saurabh', 'mishra'), ('manish', 'dhull')] \n",
      "\n",
      "PySpark RDD ZIP function results in : \n",
      " ['aman samra', 'saurabh mishra', 'manish dhull'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_name = ['aman', 'saurabh', 'manish']\n",
    "sur_name = ['samra', 'mishra', 'dhull']\n",
    "full_name = list(zip(first_name,sur_name))\n",
    "print(f'Python ZIP function results in : \\n {full_name} \\n')\n",
    "full_name = [' '.join(i) for i in full_name]\n",
    "print(f'Python ZIP function results in : \\n {full_name} \\n')\n",
    "\n",
    "print('****'*15)\n",
    "\n",
    "firstname_rdd = sc.parallelize(first_name)\n",
    "surname_rdd = sc.parallelize(sur_name)\n",
    "full_name = firstname_rdd.zip(surname_rdd)\n",
    "print(f'[PySpark RDD ZIP function results in : \\n {full_name.collect()} \\n')\n",
    "full_name = [' '.join(i) for i in full_name.collect()]\n",
    "print(f'PySpark RDD ZIP function results in : \\n {full_name} \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JOINS\n",
    "- Inner Join - Join the the RDD's on basis of KEYS\n",
    "- Left Join - Take all values from Left RDD and matches on KEY, if it doesn't find a matching key, it returns None\n",
    "- Right Join -Take all values from Right RDD and matches on KEY, if it doesn't find a matching key, it returns None\n",
    "- If input is K:V , then output can be k:(v:v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aman', 40), ('saurab', 60), ('manis', 50)]\n",
      "[('deep', 40), ('sharma', 60), ('aman', 40)]\n",
      "***************************************************************************\n",
      "INNER join on 2 rdd by key results in : [('aman', (40, 40))] \n",
      "\n",
      "LEFT join on 2 rdd by key results in : [('saurab', (60, None)), ('manis', (50, None)), ('aman', (40, 40))] \n",
      "\n",
      "RIGHT join on 2 rdd by key results in : [('sharma', (None, 60)), ('aman', (40, 40)), ('deep', (None, 40))] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1 = ['aman saurab manis']\n",
    "list2 = ['deep sharma aman']\n",
    "list1_str = sc.parallelize(list1)  \n",
    "rdd_1 = list1_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,len(x)*10))\n",
    "\n",
    "list2_str = sc.parallelize(list2)  \n",
    "rdd_2 = list2_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,len(x)*10))\n",
    "\n",
    "print(rdd_1.collect())\n",
    "print(rdd_2.collect())\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "inner_join_key = rdd_1.join(rdd_2).collect()\n",
    "left_join_key = rdd_1.leftOuterJoin(rdd_2).collect()\n",
    "right_join_key = rdd_1.rightOuterJoin(rdd_2).collect()\n",
    "\n",
    "print(f'INNER join on 2 rdd by key results in : {inner_join_key} \\n')\n",
    "print(f'LEFT join on 2 rdd by key results in : {left_join_key} \\n')\n",
    "print(f'RIGHT join on 2 rdd by key results in : {right_join_key} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aman', 40), ('saurab', 60), ('manis', 50), ('aman', 40)]\n",
      "[('deep', 40), ('sharma', 60), ('aman', 40)]\n",
      "***************************************************************************\n",
      "INNER join on 2 rdd by key results in : [('aman', (40, 40)), ('aman', (40, 40))] \n",
      "\n",
      "LEFT join on 2 rdd by key results in : [('saurab', (60, None)), ('manis', (50, None)), ('aman', (40, 40)), ('aman', (40, 40))] \n",
      "\n",
      "RIGHT join on 2 rdd by key results in : [('sharma', (None, 60)), ('aman', (40, 40)), ('aman', (40, 40)), ('deep', (None, 40))] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list1 = ['aman saurab manis aman']\n",
    "list2 = ['deep sharma aman']\n",
    "list1_str = sc.parallelize(list1)  \n",
    "rdd_1 = list1_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,len(x)*10))\n",
    "\n",
    "list2_str = sc.parallelize(list2)  \n",
    "rdd_2 = list2_str.flatMap(lambda x : x.split(' '))\\\n",
    "                            .map(lambda x : (x,len(x)*10))\n",
    "\n",
    "print(rdd_1.collect())\n",
    "print(rdd_2.collect())\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "inner_join_key = rdd_1.join(rdd_2)\n",
    "left_join_key = rdd_1.leftOuterJoin(rdd_2)\n",
    "right_join_key = rdd_1.rightOuterJoin(rdd_2)\n",
    "\n",
    "print(f'INNER join on 2 rdd by key results in : {inner_join_key.collect()} \\n')\n",
    "print(f'LEFT join on 2 rdd by key results in : {left_join_key.collect()} \\n')\n",
    "print(f'RIGHT join on 2 rdd by key results in : {right_join_key.collect()} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aman', (40, 40)), ('aman', (40, 40))]\n",
      "[('aman', (40, 40, 40, 40))]\n",
      "[('aman', <pyspark.resultiterable.ResultIterable object at 0x7f68d42ed5f8>)]\n",
      "['aman', 'aman']\n",
      "[40, 40]\n",
      "[('aman', 40), ('aman', 40)]\n",
      "[('aman', 80)]\n"
     ]
    }
   ],
   "source": [
    "print(inner_join_key.collect())\n",
    "print(inner_join_key.reduceByKey(lambda x,y : (x + y)).collect())\n",
    "print(inner_join_key.groupByKey().collect())\n",
    "\n",
    "\n",
    "print(inner_join_key.map(lambda x : (x[0])).collect())\n",
    "print(inner_join_key.map(lambda x : (x[1][1])).collect())\n",
    "print(inner_join_key.map(lambda x : (x[0] , x[1][1])).collect())\n",
    "\n",
    "print(inner_join_key.map(lambda x : (x[0] , x[1][1]))\\\n",
    "                    .reduceByKey(lambda x,y : (x + y)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CoGroup\n",
    "- Very similar to relation database operation FULL OUTER JOIN\n",
    "- Joins in spark are actually implemented with cogroup, basically the join just breaks the iterables from cogroup into tuples.\n",
    "- Instead of flattening the result per line per record, Cogroup will give you the interable interface to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hive ([5], [6])\n",
      "d ([], [5])\n",
      "hdfs ([2], [3])\n",
      "spark ([1], [2])\n",
      "teradata ([], [7])\n",
      "flink ([3], [4])\n",
      "kafka ([4], [])\n",
      "***************************************************************************\n",
      "['hive', 'd', 'hdfs', 'spark', 'teradata', 'flink', 'kafka']\n",
      "***************************************************************************\n",
      "[(<pyspark.resultiterable.ResultIterable object at 0x7fc3942c5780>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5e48>), (<pyspark.resultiterable.ResultIterable object at 0x7fc3942c5978>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5b38>), (<pyspark.resultiterable.ResultIterable object at 0x7fc3942c5e10>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5470>), (<pyspark.resultiterable.ResultIterable object at 0x7fc3942c57f0>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5048>), (<pyspark.resultiterable.ResultIterable object at 0x7fc3942c5710>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5f28>), (<pyspark.resultiterable.ResultIterable object at 0x7fc3942c5c50>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5588>), (<pyspark.resultiterable.ResultIterable object at 0x7fc3942c5400>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5518>)]\n",
      "***************************************************************************\n",
      "[<pyspark.resultiterable.ResultIterable object at 0x7fc3942c55c0>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c59b0>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5668>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c56a0>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5208>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c5e80>, <pyspark.resultiterable.ResultIterable object at 0x7fc3942c53c8>]\n",
      "***************************************************************************\n",
      "[5, 0, 2, 1, 0, 3, 4]\n",
      "***************************************************************************\n",
      "[6, 5, 3, 2, 7, 4, 0]\n",
      "***************************************************************************\n",
      "[11, 5, 5, 3, 7, 7, 4]\n",
      "***************************************************************************\n",
      "[('hive', 11), ('d', 5), ('hdfs', 5), ('spark', 3), ('teradata', 7), ('flink', 7), ('kafka', 4)]\n"
     ]
    }
   ],
   "source": [
    "tmp_list1 = [('spark', 1), ('hdfs', 2), ('flink', 3), ('kafka', 4), ('hive', 5)]\n",
    "tmp_list2 = [('spark', 2), ('hdfs', 3), ('flink', 4), ('d', 5), ('hive', 6),('teradata', 7)]\n",
    "\n",
    "tmp_list1_rdd = sc.parallelize(tmp_list1)\n",
    "tmp_list2_rdd = sc.parallelize(tmp_list2)\n",
    "\n",
    "cogroup_rdd = tmp_list1_rdd.cogroup(tmp_list2_rdd)\n",
    "\n",
    "for k,v in cogroup_rdd.collect():\n",
    "    print(k ,  tuple(map(list,v)))\n",
    "\n",
    "print('*****'*15)\n",
    "\n",
    "print(cogroup_rdd.map(lambda x : x[0]).collect())   # Returns all Keys\n",
    "print('*****'*15)\n",
    "print(cogroup_rdd.map(lambda x : x[1]).collect())   # Returns all iterables\n",
    "print('*****'*15)\n",
    "print(cogroup_rdd.map(lambda x : x[1][0]).collect())   # Returns all iterables\n",
    "print('*****'*15)\n",
    "print(cogroup_rdd.map(lambda x : sum(x[1][0])).collect())   # Returns all elements from tuple 1st part\n",
    "print('*****'*15)\n",
    "print(cogroup_rdd.map(lambda x : sum(x[1][1])).collect())   # Returns all elements from tuple 2nd part\n",
    "print('*****'*15)\n",
    "print(cogroup_rdd.map(lambda x : (sum(x[1][0])) + sum(x[1][1])).collect())   # Returns all elements and sum tuples\n",
    "print('*****'*15)\n",
    "print(cogroup_rdd.map(lambda x : (x[0], (sum(x[1][0])) + sum(x[1][1]))).collect())   # Aggregations : Returns all elements and sum tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cartesian Join\n",
    "- Here each element/key will be multiplied with every element/key of other RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('spark', 1), ('spark', 2)), (('spark', 1), ('hdfs', 3)), (('spark', 1), ('flink', 4)), (('spark', 1), ('d', 5)), (('spark', 1), ('hive', 6)), (('spark', 1), ('teradata', 7))]\n",
      "***************************************************************************\n",
      "[(1, 2), (1, 3), (1, 4), (1, 5), (1, 6), (1, 7)]\n",
      "***************************************************************************\n",
      "[3, 4, 5, 6, 7, 8]\n",
      "***************************************************************************\n",
      "[('spark', 3), ('spark', 4), ('spark', 5), ('spark', 6), ('spark', 7), ('spark', 8)]\n"
     ]
    }
   ],
   "source": [
    "cartesian_rdd = tmp_list1_rdd.cartesian(tmp_list2_rdd)\n",
    "print(cartesian_rdd.collect()[:6])   # Fetching records of spark as a key and its cross join with other keys \n",
    "print('*****'*15)\n",
    "print(cartesian_rdd.map(lambda x : (x[0][1] , x[1][1]) ).collect()[0:6])   # Fetching tuples \n",
    "print('*****'*15)\n",
    "print(cartesian_rdd.map(lambda x : (x[0][1] + x[1][1]) ).collect()[0:6])   # Summing values in tuples\n",
    "print('*****'*15)\n",
    "print(cartesian_rdd.map(lambda x : (x[0][0], (x[0][1] + x[1][1])) ).collect()[0:6]) # Key - Summing values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REDUCE - Actions ( Aggregate, take, sum, reduce, collect, max, min, std, mean , countByKey)\n",
    "- We dont need to use collect here, as its not a transformation \n",
    "- It triggers execution of DAG and gets execute on its own RDD\n",
    "- It shuffles data from multiple partitions and reduces to a single value\n",
    "- It aggregates all values for a given key value\n",
    "- \n",
    "- collect: Dump all elements and converts the RDD into a Python list\n",
    "- count: Returns the number of elements in an RDD\n",
    "- countByValue: Outputs a dictionary of (key, n), i.e. a count, by unique value. \n",
    "- take, top: Sample a few values like Unix Head/Tail command "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n",
      "15\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "python_list = [i for i in range(1,6)]\n",
    "print(python_list)\n",
    "rdd = sc.parallelize(python_list)\n",
    "print(rdd.reduce(lambda x,y : x + y))\n",
    "print(rdd.reduce(lambda x,y : x * y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 1, 's': 1, 'm': 1, 'd': 1, 'c': 1})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = ['aman', 'saurabh', 'manish', 'deep', 'clarke']\n",
    "list_rdd = sc.parallelize(list1)\n",
    "cnt = list_rdd.keyBy(lambda x : (x[0]))\n",
    "cnt.countByKey()    # It outputs in the form of MAP \n",
    "\n",
    "\n",
    "#list_rdd.countByKey() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYTHON\n",
    "\n",
    "### list = [ ]  \n",
    "- Square Brackets can occupy more size on block. Can be appended i.e Mutable \n",
    "- append, extend, insert \n",
    "- del, remove, pop to delete elements in list\n",
    
    "### tuple = ( )  \n",
    "- Simple Brackets with less size and occupies single block .\n",
    "- It also elements of multiple types like list.\n",
    "- Its immutable like RDD.\n",
    "- Same tuple cannot be changed but we can assign same tuple with more elements to Another tuple\n",
    "- We can only create, remove , update and access (slice) the elements of Tuple\n",
    "- we cant delete individual element of tuple but we can delete entire tuple using del tuplename\n",
    
    "### dict = { }   \n",
    "- Curly Brackets, it can have both list and tuple inside it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accessing tuple - Fourth element of tuple is : 4 \n",
      "\n",
      "Updating tuple  : (1, 2, 3, 'aman', 4, 5, 'samra', 6, 7, 8, 'hello', 9, 10, 'world') \n",
      "\n",
      "Deleting ENTIRE tuple (Elements cant be deleted individually) : \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'concated_tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-0d8ea67f2fe2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mconcated_tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcated_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'concated_tuple' is not defined"
     ]
    }
   ],
   "source": [
    "tuple1 = (1,2,3,'aman',4,5,'samra')\n",
    "print(f'Accessing tuple - Fourth element of tuple is : {tuple1[4]} \\n')\n",
    "\n",
    "tuple2 = (6,7,8,'hello',9,10,'world')\n",
    "\n",
    "concated_tuple = tuple1+tuple2\n",
    "print(f'Updating tuple  : {concated_tuple} \\n')\n",
    "\n",
    "print(f'Deleting ENTIRE tuple (Elements cant be deleted individually) : \\n')\n",
    "del concated_tuple\n",
    "\n",
    "print(concated_tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strange behaviour of Tuple : <class 'int'> \n",
      "\n",
      "Strange behaviour of Tuple with COMMA : <class 'tuple'> \n",
      "\n",
      "Strange behaviour of Tuple WITHOUT assigning Simple Brackets : <class 'tuple'> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuple1 = (1)  # Here interpretor will not consider this as tuple. It will only consider as a Integer\n",
    "print(f'Strange behaviour of Tuple : {type(tuple1)} \\n')\n",
    "\n",
    "tuple1 = (1,)  # Now it has become a Tuple with a COMMA \n",
    "print(f'Strange behaviour of Tuple with COMMA : {type(tuple1)} \\n')\n",
    "\n",
    "tuple1 = 1,2,3  # Now it has become a Tuple with a COMMA \n",
    "print(f'Strange behaviour of Tuple WITHOUT assigning Simple Brackets : {type(tuple1)} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
